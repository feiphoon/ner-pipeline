{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train spaCy models on synthesised training data\n",
    "\n",
    "The instructions on setting up the config files for each training run are from here: <https://spacy.io/usage/training#config>.\n",
    "\n",
    "To populate our config files, we referred to the config files for models from: <https://github.com/nlpbook/nlpbook>\n",
    "(from downloading the ag-dataset models).\n",
    "\n",
    "The variation is on different optimizers and a few changes in learning rates. Advice on how to configure the optimizers\n",
    "for these spaCy config files is from the optimizer library here: https://thinc.ai/docs/api-optimizers\n",
    "\n",
    "To package the completed models: <https://towardsdatascience.com/drugs-ner-using-spacy-in-python-f1f3091f8f4e>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall location\n",
    "data_location = \"../../data/processed/converted_train_test_data_for_ner/\"\n",
    "\n",
    "# Paths to train, val and test data\n",
    "train_data_path = Path(data_location, \"train/train.spacy\")\n",
    "dev_data_path = Path(data_location, \"dev/dev.spacy\")\n",
    "test_data_path = Path(data_location, \"test/test.spacy\")\n",
    "\n",
    "# Path to output and results\n",
    "output_path = Path(data_location, \"output\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results_path = Path(data_location, \"results\")\n",
    "results_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model training and testing: first round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ADAM, LR 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific arguments for each model\n",
    "# Train information\n",
    "model_desc = \"adam_lr_0001\"\n",
    "training_config_path__adam__lr0001 = Path(data_location, f\"training_config/config__{model_desc}.cfg\")\n",
    "output_model_path = Path(output_path, model_desc)\n",
    "output_model_path.mkdir(parents=True, exist_ok=True)\n",
    "num_max_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "# gpu_id = -1 # use CPU. This is the default.\n",
    "\n",
    "# Test information\n",
    "best_model_path =  Path(output_model_path,\"model-best\")\n",
    "best_model_results_path = Path(results_path, model_desc)\n",
    "best_model_results_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-12-12 01:39:36,766] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev', 'training.max_epochs', 'training.max_steps', 'training.optimizer.learn_rate']\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-12-12 01:39:37,705] [INFO] Set up nlp object from config\n",
      "[2021-12-12 01:39:37,715] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 01:39:37,716] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "[2021-12-12 01:39:37,716] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-12-12 01:39:37,721] [INFO] Created vocabulary\n",
      "[2021-12-12 01:39:37,721] [INFO] Finished initializing nlp object\n",
      "[2021-12-12 01:40:58,011] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2021-12-12 01:41:35,110] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2021-12-12 01:41:35,123] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 01:41:35,125] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    127.71    0.17    0.10    1.01    0.00\n",
      "  0     200        144.06   6665.08    0.00    0.00    0.00    0.00\n",
      "  0     400        284.84   4233.40   21.78   28.96   17.45    0.22\n",
      "  0     600          5.98    690.79   53.76   57.12   50.76    0.54\n",
      "  0     800          6.98    422.40   72.27   74.18   70.47    0.72\n",
      "  0    1000        112.15    821.55   74.09   74.80   73.40    0.74\n",
      "  0    1200         29.61    265.92   86.20   86.53   85.87    0.86\n",
      "  0    1400         14.49    235.81   92.01   92.06   91.96    0.92\n",
      "  0    1600          8.24    105.39   94.15   93.99   94.32    0.94\n",
      "  0    1800         10.45     98.43   96.29   96.14   96.45    0.96\n",
      "  0    2000          9.86     69.76   97.05   96.70   97.41    0.97\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../../data/output/adam_lr_0001/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train $training_config_path__adam__lr0001 \\\n",
    "    --output $output_model_path \\\n",
    "    --paths.train $train_data_path \\\n",
    "    --paths.dev $dev_data_path \\\n",
    "    --training.max_epochs $num_max_epochs \\\n",
    "    --training.max_steps 2000 \\\n",
    "    --training.optimizer.learn_rate $learning_rate \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   3.77  \n",
      "NER R   3.70  \n",
      "NER F   3.74  \n",
      "SPEED   3729  \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "                    P      R      F\n",
      "scientific       7.41   8.33   7.84\n",
      "common           0.00   0.00   0.00\n",
      "pharmaceutical   0.00   0.00   0.00\n",
      "\n",
      "\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\n",
      "../../data/results/adam_lr_0001\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate $best_model_path $test_data_path -dp $best_model_results_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 SGD, LR 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific arguments for each model\n",
    "# Train information\n",
    "model_desc = \"sgd_lr_0001\"\n",
    "training_config_path__sgd__lr0001 = Path(data_location, f\"training_config/config__{model_desc}.cfg\")\n",
    "output_model_path = Path(output_path, model_desc)\n",
    "output_model_path.mkdir(parents=True, exist_ok=True)\n",
    "num_max_epochs = 10\n",
    "learning_rate = 0.001\n",
    "# gpu_id = -1 # use CPU. This is the default.\n",
    "\n",
    "# Test information\n",
    "best_model_path =  Path(output_model_path,\"model-best\")\n",
    "best_model_results_path = Path(results_path, model_desc)\n",
    "best_model_results_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-12-12 03:16:12,270] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev', 'training.max_epochs', 'training.max_steps', 'training.optimizer.learn_rate']\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-12-12 03:16:13,179] [INFO] Set up nlp object from config\n",
      "[2021-12-12 03:16:13,188] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 03:16:13,189] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "[2021-12-12 03:16:13,190] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-12-12 03:16:13,194] [INFO] Created vocabulary\n",
      "[2021-12-12 03:16:13,194] [INFO] Finished initializing nlp object\n",
      "[2021-12-12 03:17:24,468] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2021-12-12 03:18:12,611] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2021-12-12 03:18:12,620] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 03:18:12,621] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    127.71    0.00    0.00    0.00    0.00\n",
      "  0     200        128.24  19166.63    0.00    0.00    0.00    0.00\n",
      "  0     400         97.00   1586.10    0.00    0.00    0.00    0.00\n",
      "  0     600       2023.16   1995.03    0.00    0.00    0.00    0.00\n",
      "  0     800         52.15   1316.30    0.00    0.00    0.00    0.00\n",
      "  0    1000       1472.31   1611.93    0.00    0.00    0.00    0.00\n",
      "  0    1200        959.78   1520.93    0.00    0.00    0.00    0.00\n",
      "  0    1400       2968.05   1953.70    0.00    0.00    0.00    0.00\n",
      "  0    1600       6163.79   2785.32    0.00    0.00    0.00    0.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../../data/output/sgd_lr_0001/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train $training_config_path__sgd__lr0001 \\\n",
    "    --output $output_model_path \\\n",
    "    --paths.train $train_data_path \\\n",
    "    --paths.dev $dev_data_path \\\n",
    "    --training.max_epochs $num_max_epochs \\\n",
    "    --training.max_steps 2000 \\\n",
    "    --training.optimizer.learn_rate $learning_rate \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   0.00  \n",
      "NER R   0.00  \n",
      "NER F   0.00  \n",
      "SPEED   5100  \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "                P      R      F\n",
      "scientific   0.00   0.00   0.00\n",
      "common       0.00   0.00   0.00\n",
      "\n",
      "/Users/fei/projects/inm363-individual-project/spacy-spike/venv/lib/python3.9/site-packages/spacy/displacy/__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n",
      "\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\n",
      "../../data/results/sgd_lr_0001\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate $best_model_path $test_data_path -dp $best_model_results_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 RADAM, LR 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific arguments for each model\n",
    "# Train information\n",
    "model_desc = \"radam_lr_0001\"\n",
    "training_config_path__radam__lr0001 = Path(data_location, f\"training_config/config__{model_desc}.cfg\")\n",
    "output_model_path = Path(output_path, model_desc)\n",
    "output_model_path.mkdir(parents=True, exist_ok=True)\n",
    "num_max_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "# gpu_id = -1 # use CPU. This is the default.\n",
    "\n",
    "# Test information\n",
    "best_model_path =  Path(output_model_path,\"model-best\")\n",
    "best_model_results_path = Path(results_path, model_desc)\n",
    "best_model_results_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-12-12 11:37:43,882] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev', 'training.max_epochs', 'training.max_steps', 'training.optimizer.learn_rate']\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-12-12 11:37:44,908] [INFO] Set up nlp object from config\n",
      "[2021-12-12 11:37:44,918] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 11:37:44,919] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "[2021-12-12 11:37:44,919] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-12-12 11:37:44,923] [INFO] Created vocabulary\n",
      "[2021-12-12 11:37:44,923] [INFO] Finished initializing nlp object\n",
      "[2021-12-12 11:39:09,366] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2021-12-12 11:39:46,792] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2021-12-12 11:39:46,805] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 11:39:46,806] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "[2021-12-12 11:39:46,811] [DEBUG] Removed existing output directory: ../../data/output/radam_lr_0001/model-best\n",
      "[2021-12-12 11:39:46,815] [DEBUG] Removed existing output directory: ../../data/output/radam_lr_0001/model-last\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    127.71    0.00    0.00    0.00    0.00\n",
      "  0     200         11.71  21398.00    0.00    0.00    0.00    0.00\n",
      "  0     400          3.56   1137.28    0.00    0.00    0.00    0.00\n",
      "  0     600        660.74   7011.83    0.00    0.00    0.00    0.00\n",
      "  0     800          3.96    994.17   15.13   21.24   11.75    0.15\n",
      "  0    1000          5.68    741.32   38.18   46.77   32.25    0.38\n",
      "  0    1200          7.13    492.68   61.47   67.75   56.26    0.61\n",
      "  0    1400          8.68    373.59   70.69   73.70   67.92    0.71\n",
      "  0    1600          9.47    303.77   77.11   78.62   75.66    0.77\n",
      "  0    1800         48.47    464.47   81.25   81.84   80.66    0.81\n",
      "  0    2000         69.15    640.52   87.41   88.57   86.29    0.87\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../../data/output/radam_lr_0001/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train $training_config_path__radam__lr0001 \\\n",
    "    --output $output_model_path \\\n",
    "    --paths.train $train_data_path \\\n",
    "    --paths.dev $dev_data_path \\\n",
    "    --training.max_epochs $num_max_epochs \\\n",
    "    --training.max_steps 2000 \\\n",
    "    --training.optimizer.learn_rate $learning_rate \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   21.74 \n",
      "NER R   18.52 \n",
      "NER F   20.00 \n",
      "SPEED   5036  \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "                 P       R       F\n",
      "scientific   29.63   33.33   31.37\n",
      "common       10.53    6.67    8.16\n",
      "\n",
      "/Users/fei/projects/inm363-individual-project/spacy-spike/venv/lib/python3.9/site-packages/spacy/displacy/__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n",
      "\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\n",
      "../../data/results/radam_lr_0001\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate $best_model_path $test_data_path -dp $best_model_results_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model training and testing: second round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ADAM, LR 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific arguments for each model\n",
    "# Train information\n",
    "model_desc = \"adam_lr_00001\"\n",
    "training_config_path__adam__lr00001 = Path(data_location, f\"training_config/config__{model_desc}.cfg\")\n",
    "output_model_path = Path(output_path, model_desc)\n",
    "output_model_path.mkdir(parents=True, exist_ok=True)\n",
    "num_max_epochs = 10\n",
    "learning_rate = 0.00001\n",
    "# gpu_id = -1 # use CPU. This is the default.\n",
    "\n",
    "# Test information\n",
    "best_model_path =  Path(output_model_path,\"model-best\")\n",
    "best_model_results_path = Path(results_path, model_desc)\n",
    "best_model_results_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-12-12 15:13:03,522] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev', 'training.max_epochs', 'training.max_steps', 'training.optimizer.learn_rate']\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-12-12 15:13:04,508] [INFO] Set up nlp object from config\n",
      "[2021-12-12 15:13:04,518] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 15:13:04,519] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "[2021-12-12 15:13:04,519] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-12-12 15:13:04,522] [INFO] Created vocabulary\n",
      "[2021-12-12 15:13:04,522] [INFO] Finished initializing nlp object\n",
      "[2021-12-12 15:14:18,637] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2021-12-12 15:14:56,439] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2021-12-12 15:14:56,450] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 15:14:56,451] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "[2021-12-12 15:14:56,459] [DEBUG] Removed existing output directory: ../../data/output/adam_lr_00001/model-best\n",
      "[2021-12-12 15:14:56,463] [DEBUG] Removed existing output directory: ../../data/output/adam_lr_00001/model-last\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 1e-05\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    127.71    0.17    0.10    1.01    0.00\n",
      "  0     200         10.60  30546.95    0.00    0.00    0.00    0.00\n",
      "  0     400         18.31   7558.17    0.00    0.00    0.00    0.00\n",
      "  0     600        104.51   1874.58    0.00    0.00    0.00    0.00\n",
      "  0     800          3.91   1263.77    0.00    0.00    0.00    0.00\n",
      "  0    1000         73.32   1488.83    0.00    0.00    0.00    0.00\n",
      "  0    1200        120.49   1504.72    0.00    0.00    0.00    0.00\n",
      "  0    1400       2263.18  13033.46    0.00    0.00    0.00    0.00\n",
      "  0    1600        981.46  11652.67    0.00    0.00    0.00    0.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../../data/output/adam_lr_00001/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train $training_config_path__adam__lr00001 \\\n",
    "    --output $output_model_path \\\n",
    "    --paths.train $train_data_path \\\n",
    "    --paths.dev $dev_data_path \\\n",
    "    --training.max_epochs $num_max_epochs \\\n",
    "    --training.max_steps 2000 \\\n",
    "    --training.optimizer.learn_rate $learning_rate \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   0.00  \n",
      "NER R   0.00  \n",
      "NER F   0.00  \n",
      "SPEED   3988  \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "                P      R      F\n",
      "scientific   0.00   0.00   0.00\n",
      "common       0.00   0.00   0.00\n",
      "\n",
      "\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\n",
      "../../data/results/adam_lr_00001\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate $best_model_path $test_data_path -dp $best_model_results_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 RADAM, LR 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific arguments for each model\n",
    "# Train information\n",
    "model_desc = \"radam_lr_00001\"\n",
    "training_config_path__radam__lr00001 = Path(data_location, f\"training_config/config__{model_desc}.cfg\")\n",
    "output_model_path = Path(output_path, model_desc)\n",
    "output_model_path.mkdir(parents=True, exist_ok=True)\n",
    "num_max_epochs = 10\n",
    "learning_rate = 0.00001\n",
    "# gpu_id = -1 # use CPU. This is the default.\n",
    "\n",
    "# Test information\n",
    "best_model_path =  Path(output_model_path,\"model-best\")\n",
    "best_model_results_path = Path(results_path, model_desc)\n",
    "best_model_results_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-12-12 13:22:19,772] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev', 'training.max_epochs', 'training.max_steps', 'training.optimizer.learn_rate']\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-12-12 13:22:20,578] [INFO] Set up nlp object from config\n",
      "[2021-12-12 13:22:20,587] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 13:22:20,588] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "[2021-12-12 13:22:20,588] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-12-12 13:22:20,592] [INFO] Created vocabulary\n",
      "[2021-12-12 13:22:20,592] [INFO] Finished initializing nlp object\n",
      "[2021-12-12 13:23:35,808] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2021-12-12 13:24:04,432] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2021-12-12 13:24:04,443] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 13:24:04,445] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 1e-05\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    127.71    0.00    0.00    0.00    0.00\n",
      "  0     200          0.22  32767.28    0.00    0.00    0.00    0.00\n",
      "  0     400          5.58  35553.61    0.00    0.00    0.00    0.00\n",
      "  0     600         29.34  25975.61    0.00    0.00    0.00    0.00\n",
      "  0     800         13.83   4632.16    0.00    0.00    0.00    0.00\n",
      "  0    1000         38.22   1551.92    0.00    0.00    0.00    0.00\n",
      "  0    1200         51.37   1579.82    0.00    0.00    0.00    0.00\n",
      "  0    1400        129.50   1949.70    0.00    0.00    0.00    0.00\n",
      "  0    1600        125.44   1762.33    0.00    0.00    0.00    0.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../../data/output/radam_lr_00001/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train $training_config_path__radam__lr00001 \\\n",
    "    --output $output_model_path \\\n",
    "    --paths.train $train_data_path \\\n",
    "    --paths.dev $dev_data_path \\\n",
    "    --training.max_epochs $num_max_epochs \\\n",
    "    --training.max_steps 2000 \\\n",
    "    --training.optimizer.learn_rate $learning_rate \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   0.00  \n",
      "NER R   0.00  \n",
      "NER F   0.00  \n",
      "SPEED   4929  \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "                P      R      F\n",
      "common       0.00   0.00   0.00\n",
      "scientific   0.00   0.00   0.00\n",
      "\n",
      "/Users/fei/projects/inm363-individual-project/spacy-spike/venv/lib/python3.9/site-packages/spacy/displacy/__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n",
      "\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\n",
      "../../data/results/radam_lr_00001\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate $best_model_path $test_data_path -dp $best_model_results_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Model training and testing: third round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 ADAM, LR 0.00005\n",
    "\n",
    "patience = 2000\n",
    "max_epochs = 10\n",
    "max_steps = 3000\n",
    "eval_frequency = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific arguments for each model\n",
    "# Train information\n",
    "model_desc = \"adam_lr_00005\"\n",
    "training_config_path__adam__lr00005 = Path(data_location, f\"training_config/config__{model_desc}.cfg\")\n",
    "output_model_path = Path(output_path, model_desc)\n",
    "output_model_path.mkdir(parents=True, exist_ok=True)\n",
    "num_max_epochs = 10\n",
    "learning_rate = 0.00005\n",
    "# gpu_id = -1 # use CPU. This is the default.\n",
    "\n",
    "# Test information\n",
    "best_model_path =  Path(output_model_path,\"model-best\")\n",
    "best_model_results_path = Path(results_path, model_desc)\n",
    "best_model_results_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-12-12 17:09:20,031] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev', 'training.max_epochs', 'training.max_steps', 'training.optimizer.learn_rate']\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-12-12 17:09:20,755] [INFO] Set up nlp object from config\n",
      "[2021-12-12 17:09:20,768] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 17:09:20,769] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "[2021-12-12 17:09:20,769] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-12-12 17:09:20,772] [INFO] Created vocabulary\n",
      "[2021-12-12 17:09:20,773] [INFO] Finished initializing nlp object\n",
      "[2021-12-12 17:10:33,821] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2021-12-12 17:11:09,844] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2021-12-12 17:11:09,856] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 17:11:09,857] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 5e-05\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    127.71    0.17    0.10    1.01    0.00\n",
      "  0     300        205.33  10861.53    0.00    0.00    0.00    0.00\n",
      "  0     600        571.09   6558.80   10.99   15.18    8.62    0.11\n",
      "  0     900          7.38   1163.80   43.91   48.49   40.12    0.44\n",
      "  0    1200         10.04    752.11   68.54   71.61   65.73    0.69\n",
      "  0    1500         10.46    488.32   76.22   77.20   75.26    0.76\n",
      "  0    1800         14.43    487.84   79.74   79.84   79.65    0.80\n",
      "  0    2100         99.99    757.79   85.68   86.76   84.63    0.86\n",
      "  0    2400        172.58   1051.76   94.02   93.91   94.12    0.94\n",
      "  0    2700         23.34    289.73   96.67   96.42   96.93    0.97\n",
      "  0    3000         23.98    252.59   98.29   98.12   98.46    0.98\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../../data/output/adam_lr_00005/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train $training_config_path__adam__lr00005 \\\n",
    "    --output $output_model_path \\\n",
    "    --paths.train $train_data_path \\\n",
    "    --paths.dev $dev_data_path \\\n",
    "    --training.max_epochs $num_max_epochs \\\n",
    "    --training.max_steps 3000 \\\n",
    "    --training.optimizer.learn_rate $learning_rate \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   14.29 \n",
      "NER R   14.81 \n",
      "NER F   14.55 \n",
      "SPEED   3374  \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "                     P       R       F\n",
      "scientific       20.00   25.00   22.22\n",
      "common            8.33    6.67    7.41\n",
      "pharmaceutical    0.00    0.00    0.00\n",
      "\n",
      "/Users/fei/projects/inm363-individual-project/spacy-spike/venv/lib/python3.9/site-packages/spacy/displacy/__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n",
      "\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\n",
      "../../data/results/adam_lr_00005\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate $best_model_path $test_data_path -dp $best_model_results_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 RADAM, LR 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific arguments for each model\n",
    "# Train information\n",
    "model_desc = \"radam_lr_00005\"\n",
    "training_config_path__radam__lr00005 = Path(data_location, f\"training_config/config__{model_desc}.cfg\")\n",
    "output_model_path = Path(output_path, model_desc)\n",
    "output_model_path.mkdir(parents=True, exist_ok=True)\n",
    "num_max_epochs = 10\n",
    "learning_rate = 0.00005\n",
    "# gpu_id = -1 # use CPU. This is the default.\n",
    "\n",
    "# Test information\n",
    "best_model_path =  Path(output_model_path,\"model-best\")\n",
    "best_model_results_path = Path(results_path, model_desc)\n",
    "best_model_results_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-12-12 18:53:01,771] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev', 'training.max_epochs', 'training.max_steps', 'training.optimizer.learn_rate']\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-12-12 18:53:02,814] [INFO] Set up nlp object from config\n",
      "[2021-12-12 18:53:02,829] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 18:53:02,830] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "[2021-12-12 18:53:02,832] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-12-12 18:53:02,839] [INFO] Created vocabulary\n",
      "[2021-12-12 18:53:02,839] [INFO] Finished initializing nlp object\n",
      "[2021-12-12 18:54:21,233] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2021-12-12 18:54:55,894] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2021-12-12 18:54:55,911] [DEBUG] Loading corpus from path: ../../data/dev/dev.spacy\n",
      "[2021-12-12 18:54:55,913] [DEBUG] Loading corpus from path: ../../data/train/train.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 5e-05\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    127.71    0.00    0.00    0.00    0.00\n",
      "  0     300         17.19  33625.83    0.00    0.00    0.00    0.00\n",
      "  0     600        143.64   2451.15    0.00    0.00    0.00    0.00\n",
      "  0     900        990.39   9905.90    0.00    0.00    0.00    0.00\n",
      "  0    1200          5.47   1506.19   10.51   15.60    7.92    0.11\n",
      "  0    1500          7.17   1154.00   39.77   51.74   32.29    0.40\n",
      "  0    1800         11.30   1012.82   52.91   59.77   47.47    0.53\n",
      "  0    2100         14.50    794.24   68.07   71.53   64.93    0.68\n",
      "  0    2400        333.83   1956.84   75.23   76.81   73.72    0.75\n",
      "  0    2700         23.24    815.92   81.81   82.42   81.21    0.82\n",
      "  0    3000         30.92    661.17   91.08   91.22   90.93    0.91\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../../data/output/radam_lr_00005/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train $training_config_path__radam__lr00005 \\\n",
    "    --output $output_model_path \\\n",
    "    --paths.train $train_data_path \\\n",
    "    --paths.dev $dev_data_path \\\n",
    "    --training.max_epochs $num_max_epochs \\\n",
    "    --training.max_steps 3000 \\\n",
    "    --training.optimizer.learn_rate $learning_rate \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   24.44 \n",
      "NER R   20.37 \n",
      "NER F   22.22 \n",
      "SPEED   5063  \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "                 P       R       F\n",
      "scientific   33.33   37.50   35.29\n",
      "common       11.11    6.67    8.33\n",
      "\n",
      "/Users/fei/projects/inm363-individual-project/spacy-spike/venv/lib/python3.9/site-packages/spacy/displacy/__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n",
      "\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\n",
      "../../data/results/radam_lr_00005\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate $best_model_path $test_data_path -dp $best_model_results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7bfb39f90f2f9385df598571e0489e84f399e28151822edddb4b795d7279ab50"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
